{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/insurance.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# loading the preprocessing pipeline we built in the previous notebook\n",
    "column_transformer = joblib.load(\"../model_files/transformer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brian/Code/regression-model/venv/lib/python3.8/site-packages/featuretools/entityset/entity.py:600: UserWarning: Using first column as index. To change this, specify the index parameter\n",
      "  warnings.warn((\"Using first column as index. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.90000000e+01, 2.79000000e+01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 3.00000000e+00, 1.68849240e+04],\n",
       "       [1.80000000e+01, 3.37700000e+01, 1.00000000e+00, ...,\n",
       "        1.00000000e+00, 2.00000000e+00, 1.72555230e+03],\n",
       "       [2.80000000e+01, 3.30000000e+01, 3.00000000e+00, ...,\n",
       "        1.00000000e+00, 2.00000000e+00, 4.44946200e+03],\n",
       "       ...,\n",
       "       [1.80000000e+01, 3.68500000e+01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 2.00000000e+00, 1.62983350e+03],\n",
       "       [2.10000000e+01, 2.58000000e+01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 3.00000000e+00, 2.00794500e+03],\n",
       "       [6.10000000e+01, 2.90700000e+01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 1.00000000e+00, 2.91413603e+04]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the column transformer\n",
    "\n",
    "features = column_transformer.fit_transform(df)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dataframe Shapes:  (1070, 34) (268, 34)\n",
      "Target Dataframe Shapes:  (1070,) (268,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    df[\"charges\"], \n",
    "                                                    train_size=0.80, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print(\"Feature Dataframe Shapes: \", X_train.shape, X_test.shape)\n",
    "print(\"Target Dataframe Shapes: \", y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-974c0bae95b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save test set to be used later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/testing_predictors.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/testing_targets.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# save test set to be used later\n",
    "X_test.to_csv(\"../../data/testing_predictors.csv\")\n",
    "y_test.to_csv(\"../../data/testing_targets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_regressor = TPOTRegressor(generations=10,\n",
    "                               population_size=10,\n",
    "                               random_state=42,\n",
    "                               cv=5,\n",
    "                               n_jobs=4,\n",
    "                               verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 0.11.6.post2 of tpot is outdated. Version 0.11.6.post3 was released 1 day ago.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Optimization Progress'), FloatProgress(value=0.0, max=110.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -0.001151138157379548\n",
      "\n",
      "Generation 2 - Current best internal CV score: -0.001151138157379548\n",
      "\n",
      "Generation 3 - Current best internal CV score: -0.001151138157379548\n",
      "\n",
      "Generation 4 - Current best internal CV score: -0.001151138157379548\n",
      "\n",
      "Generation 5 - Current best internal CV score: -3.193635632661969e-22\n",
      "\n",
      "Generation 6 - Current best internal CV score: -3.193635632661969e-22\n",
      "\n",
      "Generation 7 - Current best internal CV score: -3.193635632661969e-22\n",
      "\n",
      "Generation 8 - Current best internal CV score: -1.2103502888371635e-22\n",
      "\n",
      "Generation 9 - Current best internal CV score: -1.2103502888371635e-22\n",
      "\n",
      "Generation 10 - Current best internal CV score: -1.0918663294138555e-22\n",
      "\n",
      "Best pipeline: LassoLarsCV(OneHotEncoder(RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.1, min_samples_leaf=19, min_samples_split=19, n_estimators=100), minimum_fraction=0.15, sparse=False, threshold=10), normalize=False)\n"
     ]
    }
   ],
   "source": [
    "tpot_regressor = tpot_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "\n",
    "Now that we have an optimal pipeline created by TPOT we will be adding our own preprocessors to it.\n",
    "\n",
    "First, we'll export the pipeline as a python file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"../../data/insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_regressor.export('tpot_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code that will create the pipeline that was found by the TPOT package, we'll execute it here to make sure that we can instantiate the Pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "tpot_pipeline = make_pipeline(\n",
    "    Normalizer(norm=\"l2\"),\n",
    "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=0.7500000000000001, min_samples_leaf=17, min_samples_split=9, n_estimators=100)\n",
    ")\n",
    "\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can build the same pipeline that was found by the TPOT package, we'll add our own preprocessors to the pipeline. This will ensure that the final pipeline will accept the features in the original dataset and will process the features correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll compose all of the pipelines we created above into one ColumnTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"categorical_transformers\", categorical_transformers, ['sex', 'smoker', 'region'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the ColumnTransformer to the tpot pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('tpot_pipeline', tpot_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline\n",
    "\n",
    "To make sure that the pipeline works as expected we'll fit it and make a prediction using data from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8828053733667738\n"
     ]
    }
   ],
   "source": [
    "predictions = tpot_regressor.predict(X_test)\n",
    "print(r2_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Unfitted Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tpot_regressor.fitted_pipeline_, open(\"../model_files/model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot_regressor.export('tpot_exported_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<deap.gp.Primitive at 0x1262deb30>,\n",
       " <deap.gp.Primitive at 0x1262f8900>,\n",
       " <deap.gp.Primitive at 0x1262fba40>,\n",
       " <deap.gp.Terminal at 0x1262dd180>,\n",
       " <deap.gp.Terminal at 0x1262ffac0>,\n",
       " <deap.gp.Terminal at 0x1262f9380>,\n",
       " <deap.gp.Terminal at 0x1262e7900>,\n",
       " <deap.gp.Terminal at 0x1262e7d80>,\n",
       " <deap.gp.Terminal at 0x1262ea380>,\n",
       " <deap.gp.Terminal at 0x1262ea6c0>,\n",
       " <deap.gp.Terminal at 0x1262ea9c0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot_regressor._optimized_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
